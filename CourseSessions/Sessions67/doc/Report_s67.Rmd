<link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
<style type="text/css"> body {padding: 10px 30px 10px 30px;} table,th, td {text-align: center;}</style>
<style>
td.tableRow
{
text-align:center;
}
</style>


Classification Methods
========================================================

**T. Evgeniou, INSEAD**

What is this for?
---------------------------------------------------------

A bank is interested in knowing which customers are likely to default on loan payments. The bank is also interested in knowing what characteristics of customers will be useful to explain their loan payment behavior. An advertiser is interested in choosing the set of customers or prospects who are most likely to respond to a direct mail campaign. The advertiser is also interested in knowing what characteristics of consumers are most likely to explain responsiveness to a direct mail campaign. A procurement manager is interested in knowing which orders will be delayed. An investor is interested in knowing which assets are most likely to increase in value. 

Classification (or categorization) techniques are useful to help answer such questions. They help predict the group membership (or class - hence called **classification techniques**) of individuals (data), for **predefined group memberships**, and also to describe which characteristics of individuals can predict their group membership. Examples of group memberships/classes could be: (1) loyal customers versus customers who will churn; (2) high price sensitive versus low price sensitive customers; (3) satisfied versus dissatisfied customers; (4) purchasers versus non-purchasers; (5) assets tha increase in value versus not; (6) products that may be good recommendations to a customer versus not, etc. Characteristics that are useful in classifying individuals/data into predefined groups/classes could include for example (1) demographics; (2) psychographics; (3) past behavior; (4) attitudes towards specific products, etc. The choice of characteristics for classification should be based on the usefulness for solving the managerial problem in hand and on its ability to discriminate between the two groups.



There are many techniques for solving classification problems: classification trees, logistic regression, discriminant analysis, neural networks, boosted trees, random forest, deep learning methods (an area Facebook <a href="https://sites.google.com/site/deeplearningworkshopnips2013/schedule">  (Zuckerberg recently in an academic conference as a panelist, too) </a>  has <a href="http://www.wired.com/wiredenterprise/2013/12/facebook-yann-lecun-qa/"> invested in</a>) nearest neighbor, support vector machines, etc, (e.g. see the R packages "e1071" for more example methods). In this report, for simplicity  we focus on the first two, although one can always use some of the other methods instead of the ones discussed here. The focus of this note is not do describe any specific ("black box, math") classification method, but to describe a process for classification independent of the method used (e.g. independent of the method selected in one of the steps in the process outlined below).

An important question when using classification methods is to assess the relative performance of all available ("black box") methods i.e. in order to use the best one according to our criteria. To this purpose there are standard performance **classification assessment metrics**, which we discuss below - this is a key focus of this note.  


Classification using an Example
--------------------------------------------


### The "Business Decision"" 

A boating company had become a victim of the crisis in the boating industry. The business problem of the "Boat" case study, although hypothetical, depicts very well the sort of business problems faced by many real companies in an increasingly data-intensive business environment. The management team was now exploring various growth options. Expanding further in some markets, in particular North America, was no longer something to consider for the distant future. It was becoming an immediate necessity. 

The team believed that in order to develop a strategy for North America, they needed a better understanding of their current and potential customers in that market. They believed that they had to build more targeted boats for their most important segments there. To that purpose, the boating company had been commissioned a project for that market. Being a data-friendly company, the decision was made to develop an understanding of their customers in a data-driven way - none of that soft hold-hands-and-discuss stuff. 

The company would like to understand who would be the most likely customers to purchase a boat in the future or to recommend their brand, as well as what would be the **key purchase drivers** that affect people's decision to purchase or recommend. 

### The Data

With the aid of a market research firm, the boating company gathered various data about the boating market in the US through interviews with almost 3,000 boat owners and intenders. The data consisted, among others, 29 attitudes towards boating, which respondents indicated on a 5-point scale (Q. 1, 1-29 shown in the survey in the appendix).

Other types of information had been collected, such as demographics (Q. 11-15 in the survey). Finally, the boating company had compiled information about the boats, such as the length of the boat they owned, how they used their boats, and the price of the boats (Q. 2-10 in the survey). 

After analyzing the survey data (using for example factor and cluster analysis), the company managers decided to only focus on a few purchase drivers which they thought were the most important ones. They decided to perform the classification and purchase drivers analysis using only the responses to the following questions:

1. "Q16_2_Has.best.in.class.customer.service"                                    
2. "Q16_5_Is.a.leader.in.safety"                                                 
3. "Q16_8_Is.a.good.brand.for.people.that.are.new.to.boating"                    
4. "Q16_10_Offers.boats.that.provide.a.fast.and.powerful.boating.experience"     
5. "Q16_11_Offers.the.best.boats.for.socializing"                                
6. "Q16_12_Offers.the.best.boats.for.water.sports..e.g...tubing..ski..wakeboard."
7. "Q16_13_Offers.boats.with.superior.interior.style"                            
8. "Q16_17_Offers.boats.that.can.handle.rough.weather.or.choppy.water"           
9. "Q16_18_Offers.boats.that.can.handle.frequent.and.heavy.usage"                
10. "Q16_21_Offers.boats.that.are.easy.to.maintain.and.or.repair"                 
11. "Q16_22_Offers.boats.that.are.easy.to.use"                                    
12. "Q16_24_Has.low.prices"                                                       
13. "Q16_25_Is.a.brand.that.gives.me.peace.of.mind"                               
14. "Q16_27_Is.a.brand.that.impresses.others"                                     
15. "Q17_Recommend"                                                               
16. "Q18_PurchaseFuture" 

Let's get the data and see it for a few customers. This is how the first `r min(max_data_report, nrow(ProjectData))` out of the total of `r nrow(ProjectData)` rows look:
<br>

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
# let's make the data into data.matrix classes so that we can easier visualize them
ProjectData = data.matrix(ProjectData)
```


<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
show_data = data.frame(round(ProjectData,2))
show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=400,allowHTML=TRUE,page='disable'))
print(m1,'chart')
```
</div>
</div>
<br> <br>

We will see some descriptive statistics of the data later, when we get into statistical analysis.

### A Process for Classification

<blockquote> <p>
It is important to remember that Data Analytics Projects require a delicate balance between experimentation, intuition, but also following (once a while) a process to avoid getting fooled by randomness in data and finding "results and patterns" that are mainly driven by our own biases and not by the facts/data themselves.
</p> </blockquote>

There is *not a single best* process for classification. However, we have to start somewhere, so we will use the following process:

#### Classification in 6 steps

1. Create an estimation sample and two validation samples by splitting the data into three groups. Steps 2-5 below will then be performed only on the estimation and the first validation data. You should only do step 6 once on the second validation data, also called **test data**, and report/use the performance on that (second validation) data only to make final business decisions. 

2.  Set up the dependent variable (as a categorical 0-1 variable; multi-class classification is also feasible, and similar, but we do not explore it in this note). 

3. Make a preliminary assessment of the relative importance of the explanatory variables using visualization tools and simple descriptive statistics. 

4.  Estimate the classification model using the estimation data, and interpret the results.

5. Assess the accuracy of classification in the first validation sample, possibly repeating steps 2-5 a few times in different ways to increase performance.

6. Finally, assess the accuracy of classification in the second validation sample.  You should eventually use/report all relevant performance measures/plots on this second validation sample only.

Let's follow these steps.

#### Step 1: Splitting the data into estimation and validation samples

It is very important that you finally measure and report (or expect to see from the data scientists working on the project) the performance of the models on **data that have not been used at all during the analysis, called "out-of-sample" data** (steps 2-5 above). The idea is that in practice we want our models to be used for predicting the class of observations/data we have not seen yet (e.g. "the future data"): although the performance of a classification method may be high in the data used to estimate the model parameters, it may be significantly poorer on data not used for parameter estimation, such as the **out-of-sample** (future) data in practice. The second validation data mimic such out-of-sample data, and the performance on this validation set is a better approximation of the performance one should expect in practice from the selected classification method.  This is why we split the data into an estimation sample and two validation samples  - using some kind of randomized splitting technique.  The estimation data and the first validation data are used during steps 2-5 (with a few iterations of these steps), while the second validation data is only used once at the very end before making final business decisions based on the analysis. The split can be, for example, 80% estimation, 10% validation 1, and 10% validation 2, depending on the number of observations - for example, when there is a lot of data, you may only keep a few hundreds of them for each validation set, and use the rest for estimation. 

While setting up the estimation and validation samples, you should also check that the same proportion of data from each class, i.e. people who plan to purchase a boat versus not, are maintained in each sample, i.e., you should maintain the same balance of the dependent variable categories as in the overall dataset. 

For simplicy, in this note we will  not iterate steps 2-5.  Again, this should **not** be done in practice, as we should usually iterate steps 2-5 a number of times using the first validation sample each time, and make our final assessment of the classification model using the second validation sample only once (ideally). 

```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='hide'}

if (random_sampling){
  estimation_data=sample.int(nrow(ProjectData),floor(estimation_data_percent*nrow(ProjectData)/100))
  non_estimation_data = setdiff(1:nrow(ProjectData),estimation_data)
  validation_data=non_estimation_data[sample.int(length(non_estimation_data), floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))]
  } else {
    estimation_data=1:floor(estimation_data_percent*nrow(ProjectData)/100)
    non_estimation_data = setdiff(1:nrow(ProjectData),estimation_data)
    validation_data = (tail(estimation_data,1)+1):(tail(estimation_data,1) + floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))
    }

test_data = setdiff(1:nrow(ProjectData), union(estimation_data,validation_data))

estimation_data=ProjectData[estimation_data,]
validation_data=ProjectData[validation_data,]
test_data=ProjectData[test_data,]
```

We typically call the three data samples as **estimation_data** (e.g. `r estimation_data_percent`% of the data in our case),  **validation_data**  (e.g. the `r validation_data_percent`% of the data) and **test_data** (e.g. the remaining `r 100 - estimation_data_percent  -  validation_data_percent`% of the data).

There are `r nrow(estimation_data)` observations in the estimation data, 
`r nrow(validation_data)` in the validation data, and `r nrow(test_data)` in the test data. 

#### Step 2: Setting up the dependent variable

First, make sure the dependent variable is set up as a categorical 0-1 variable. In this illustrative example, the intent to purchase or recommend variable is a 0-1 variable and this is what will be our dependent variable. The data however may not be always readily available with a categorical dependent variable. Suppose a retail store wants to understand what discriminates consumers who are  loyal versus those who are not. If they have data on the amount of  dollars that customers spend in their store, they can create a categorical variable ("loyal vs not-loyal") by using a definition such as: "A loyal customer is one who spends more than X amount at their store". They can then code these loyal customers as "1" and the others as "0". They can choose the threshold X as they wish, which may have a big impact in the overall analysis. This decision can be the most crucial one of the whole data analysis: a wrong choice at this step may lead both to poor performance later as well as to no valuable insights. One should revisit the choice made at this step several times, iterating steps 2-3 and 2-5.

<blockquote> <p>
Carefully deciding what the dependent 0/1 variable is can be the most critical choice of a classification analysis. This decision typically depends on contextual knowledge and needs to be revisited multiple times throughout a data analytics project. 
</p> </blockquote>

In our data the percentages of 0/1's in our estimation sample is as follows:
<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
class_percentages=matrix(c(sum(estimation_data[,dependent_variable]==1),sum(estimation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Class 1", "Class 0")
rownames(class_percentages)<-"# of Observations"
print(xtable(class_percentages ,caption=paste("Number of Observations per class in the Estimation Sample:",data_name,sep=" "), digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)
```
</div>
</div>
while in the validation sample these percentages are:

<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
class_percentages=matrix(c(sum(validation_data[,dependent_variable]==1),sum(validation_data[,dependent_variable]==0)), nrow=1); colnames(class_percentages)<-c("Class 1", "Class 0")
rownames(class_percentages)<-"# of Observations"
print(xtable(class_percentages ,caption=paste("Number of Observations per class in the Validation Sample:",data_name,sep=" "), digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)
```
</div>
</div>


#### Step 3: Make a preliminary assessment of the relative importance of the explanatory variables using visualization tools and simple descriptive statistics.

Good data analytics starts with good contextual knowledge as well as a simple statistical and visualization exploration of the data. In the case of classification, one can explore "simple classifications" by assessing how the  classes differ along any of the independent variables. For example, these are the statistics of our independent variables across the two classes, class 1, "purchase" (first table), and class 0, "no purchase" (second table):


<div class="row">
<div class="col-md-6">
```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}  
show_data = data.frame(round(my_summary(estimation_data[estimation_data[,dependent_variable]==1,]),2))
#show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=400,allowHTML=TRUE,page='disable'))
print(m1,'chart')
```
</div>
</div>

<div class="row">
<div class="col-md-6">
```{r echo=FALSE, comment=NA, warning=FALSE, message=FALSE,results='asis'}  
show_data = data.frame(round(my_summary(estimation_data[estimation_data[,dependent_variable]==0,]),2))
#show_data = show_data[1:min(max_data_report,nrow(show_data)),]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-"Variables"
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=400,allowHTML=TRUE,page='disable'))
print(m1,'chart')
```
</div>
</div>


The purpose of such an analysis by class is to get an initial idea about whether the classes are indeed separable as well as to understant which of the independent variables have most discriminatory power. Can you see any differences across the two classes in the tables above? 

Notice however that:

<blockquote> <p>
Even though each independent variable may not differ across classes, classification may still be feasible as a (linear or nonlinear) combination of independent variables may still be discriminatory. 
</p> </blockquote>

A simple visualization tool to assess the discriminatory power of the independent variables are the **box plots**. These visually indicate simple summary statistics of an independent variable (e.g. mean, median, )
For example consider the box plots for our data for the first 4 independent variables are:
<center style="width=1048px;">
```{r echo=FALSE, message=FALSE, warning=FALSE,prompt=FALSE, results='asis',fig.height=10,fig.width=16}
split.screen(c(2, 2))
screen(1); boxplot(estimation_data[,independent_variables[1]]~estimation_data[,dependent_variable],data=estimation_data, main=independent_variables[1], xlab=dependent_variable)
screen(2); boxplot(estimation_data[,independent_variables[2]]~estimation_data[,dependent_variable],data=estimation_data, main=independent_variables[2], xlab=dependent_variable)
screen(3); boxplot(estimation_data[,independent_variables[3]]~estimation_data[,dependent_variable],data=estimation_data, main=independent_variables[3], xlab=dependent_variable)
screen(4); boxplot(estimation_data[,independent_variables[4]]~estimation_data[,dependent_variable],data=estimation_data, main=independent_variables[4], xlab=dependent_variable)
```
</center>
Can you see which variables appear to be most discrimatory?



#### Step 4: Estimate the classification model using the estimation data, and interpret the results.

Once we decide which  dependent and independent variables to use, one can use a number of classification methods to develop a model that discriminated the different classes. 

<blockquote> <p>
Some of the widely used classification methods are:  classification and regression trees, boosted trees,support vector machines, neural networks, nearest neighbors, logistic regression, lasso, random forests, deep learning methods, etc.
</p> </blockquote>

In this note we will consider only two classification methods: **logistic regression** and **classification and regression trees (CART)**. However, replacing them with other methods is simple. Understanding how these methods work is beyond the scope of this note - there are many references available online for all these classification methods. 

CART is a widely used classification method largely because the estimated classification models are easy to interpret. This classification tool iteratively "splits" the data using the most discriminatory independent variable at each step, building a "tree" - as shown below - on the way. The CART methods **limit the size of the tree** using various statistical techniques in order to avoid **overfitting the data**. For example, using the rpart and rpart.control functions in R, we can limit the size of the try by selecting the functions' **complexity control** paramater **cp** (what this does it beyond the scope of this note. For the rpart and rpart.control functions in R, smaller values, e.g. cp=0.001, lead to larger trees that larger values, e.g. cp = 0.01, as we will see next).

<blockquote> <p>
One of the biggest risks of developing classification models is overfitting: while it is always trivial to develop a model (e.g. a tree) that classifies any (estimation) dataset with no misclassification error at all, there is no guarantee that the quality of a classifier in out-of-sample data (e.g. in the validation data) will be close to that in the estimation data. Finding the right balance between "over-fitting" and "under-fitting" is one of the most important aspects in data analytics. While there are a number of statistical techniques to help us find this balance - including the use of validation data - it is largely a combination of good statistical analysis with qualitative criteria (e.g. regarding the interpretability or simplicity of the estimated models) that leads to classification models which can work well in practice. 
</p> </blockquote>

Running a basic CART model with complexity control cp=`r CART_cp`,  leads to the following tree (**NOTE**: for better readability of the tree figures below,  we will rename the independent variables as IV1 to `r paste("IV", length(independent_variables), sep="")` when using CART):

```{r echo=FALSE, message=FALSE,warning=FALSE, prompt=FALSE, results='asis'}

# just name the variables numerically so that they look ok on the tree plots
independent_variables_nolabel = paste("IV", 1:length(independent_variables), sep="")

estimation_data_nolabel = cbind(estimation_data[,dependent_variable], estimation_data[,independent_variables])
colnames(estimation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

validation_data_nolabel = cbind(validation_data[,dependent_variable], validation_data[,independent_variables])
colnames(validation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

test_data_nolabel = cbind(test_data[,dependent_variable], test_data[,independent_variables])
colnames(test_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

estimation_data_nolabel = data.frame(estimation_data_nolabel)
validation_data_nolabel = data.frame(validation_data_nolabel)
test_data_nolabel = data.frame(test_data_nolabel)

estimation_data = data.frame(estimation_data)
validation_data = data.frame(validation_data)
test_data = data.frame(test_data)

```
<center>
```{r echo=FALSE, message=FALSE, warning=FALSE,prompt=FALSE, results='asis'}
formula=paste(colnames(estimation_data[,dependent_variable,drop=F]),paste(Reduce(paste,sapply(head(independent_variables_nolabel,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables_nolabel,1),sep=""),sep="~")
CART_tree<-rpart(formula, data= estimation_data_nolabel,method="class", control=CART_control)

fancyRpartPlot(CART_tree, main=paste("Classification Tree for", data_name,sep=" "))
```
</center>
The leaves of the tree indicate the number of estimation data observations that belong to each class which "reach that leaf". A perfect classification would only have data from one class in each of the tree leaves. However, such a perfect classification of the estimation data would most likely not be able to classify well out-of-sample data due to over-fitting the estimation data.

One can estimate larger trees through changing the tree's **complexity control** parameter (in this case the rpart.control argument cp). For example, this is how the tree would look like if we set cp = 0.005
<center style="width=900px; height=800px;">
```{r echo=FALSE, message=FALSE,warning=FALSE, prompt=FALSE, results='asis',fig.width=14,fig.height=10}
CART_tree_large<-rpart(formula, data= estimation_data_nolabel,method="class", control=rpart.control(cp = 0.005))

fancyRpartPlot(CART_tree_large, paste("Another classification Tree for", data_name,sep=" "))
```
</center>
One can use the percentage of data in each leaf of the tree to have an estimate that an observation (e.g. person) belongs to a given class. Specifically, given a new observation (e.g .person), the tree can be used to understand the leaf this observation would belong to. The **purity of the leaf** can indicate the probability an observation belongs to a class. In our case, the probability our validation data belong to class 1 (e.g. the customer is likely to purchase a boat) for the first few validation data observations, using the first CART above, is:

```{r echo=FALSE, message=FALSE,warning=FALSE, results='asis'}
# Let's first calculate all probabilites for the estimation, validation, and test data
estimation_Probability_class1_tree<-predict(CART_tree, estimation_data_nolabel)[,2]
estimation_Probability_class1_tree_large<-predict(CART_tree_large, estimation_data_nolabel)[,2]

validation_Probability_class1_tree<-predict(CART_tree, validation_data_nolabel)[,2]
validation_Probability_class1_tree_large<-predict(CART_tree_large, validation_data_nolabel)[,2]

test_Probability_class1_tree<-predict(CART_tree, test_data_nolabel)[,2]
test_Probability_class1_tree_large<-predict(CART_tree_large, test_data_nolabel)[,2]


estimation_prediction_class_tree=1*as.vector(estimation_Probability_class1_tree > Probability_Threshold)
estimation_prediction_class_tree_large=1*as.vector(estimation_Probability_class1_tree_large > Probability_Threshold)

validation_prediction_class_tree=1*as.vector(validation_Probability_class1_tree > Probability_Threshold)
validation_prediction_class_tree_large=1*as.vector(validation_Probability_class1_tree_large > Probability_Threshold)

test_prediction_class_tree=1*as.vector(test_Probability_class1_tree > Probability_Threshold)
test_prediction_class_tree_large=1*as.vector(test_Probability_class1_tree_large > Probability_Threshold)

```
<br>
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}

Classification_Table=rbind(validation_data[,dependent_variable],validation_Probability_class1_tree)
rownames(Classification_Table)<-c("Actual Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

Classification_Table_large=rbind(validation_data[,dependent_variable],validation_Probability_class1_tree)
rownames(Classification_Table_large)<-c("Actual Class","Probability of Class 1")
colnames(Classification_Table_large)<- paste("Obs", 1:ncol(Classification_Table_large), sep=" ")

show_data = data.frame(round(Classification_Table,2))
show_data = show_data[,1:min(max_data_report,ncol(show_data))]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-""
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=240,allowHTML=TRUE,page='disable'))
print(m1,'chart')

```

In practice we need to select the **probability threshold** above which we consider an observation as "class 1": this is an important choice that we will discuss below. First we discuss another method widely used, namely logistic regression.

**Logistic Regression** is a method similar to linear regression except that the dependent variable can be discrete (e.g. 0 or 1). **Linear** logistic regression estimates the coefficients of a linear regression using the selected independent variables while optimizing a classification criterion. For example, this is the logistic regression equation for our data:



```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}

formula_log=paste(colnames(estimation_data[,dependent_variable,drop=F]),paste(Reduce(paste,sapply(head(independent_variables,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables,1),sep=""),sep="~")

logreg_solution <- glm(formula_log, family=binomial(link="logit"), data=estimation_data)

log_coefficients = round(summary(logreg_solution)$coefficients,1)
print(xtable(log_coefficients,caption="Logistic Regression: Estimated Coefficients" , digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)

```

Given a set of independent variables, the output of the estimated logistic regression (the sum of the products of the independent variables with the corresponding regression coefficients) can be used to assess the probability an observation belongs to one of the classes. Specifically, the regression output can be transformed into a probability of belonging to, say, class 1 for each observation. In our case, the probability our validation data belong to class 1 (e.g. the customer is likely to visit the resort) for the first few validation data observations, using the logistic regression above, is:

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
# Let's get the probabilities for the 3 types of data again
estimation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=estimation_data[,independent_variables])
validation_Probability_class1_log<-predict(logreg_solution, type="response", newdata=validation_data[,independent_variables])
test_Probability_class1_log<-predict(logreg_solution, type="response", newdata=test_data[,independent_variables])

estimation_prediction_class_log=1*as.vector(estimation_Probability_class1_log > Probability_Threshold)
validation_prediction_class_log=1*as.vector(validation_Probability_class1_log > Probability_Threshold)
test_prediction_class_log=1*as.vector(test_Probability_class1_log > Probability_Threshold)

```

```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}

Classification_Table=rbind(validation_data[,dependent_variable],validation_Probability_class1_log)
rownames(Classification_Table)<-c("Actual Class","Probability of Class 1")
colnames(Classification_Table)<- paste("Obs", 1:ncol(Classification_Table), sep=" ")

show_data = data.frame(round(Classification_Table,2))
show_data = show_data[,1:min(max_data_report,ncol(show_data))]
row<-rownames(show_data)
dfnew<-cbind(row,show_data)
change<-colnames(dfnew)
change[1]<-""
colnames (dfnew)<-change
m1<-gvisTable(dfnew,options=list(showRowNumber=TRUE,width=1220, height=240,allowHTML=TRUE,page='disable'))
print(m1,'chart')

```

The default decision is to classify each observation in the group with the highest probability - but we will relax this below. 

Selecting the best subset of independent variables for logistic regression, a special case of the general problem of **feature selection**, is an iterative process where both the significance of the regression coefficients as well as the performance of the estimated logistic regression equation on the first validation data are used as guidance. A number of variations are tested in practice, each leading to different performance, which we discuss next. 

#### Step 5: Assess the accuracy of classification in the first validation sample


Using the predicted class probabilities  of the validation data, as outlined above, we can  generate four basic measures of classification performance. Before discussing them, note that given the probability an observation belongs to a class, **a reasonable class prediction choice is to predict the class that has the highest probability**. However, this does not need to be the only choice in practice.

<blockquote> <p>
Selecting the probability threshold based on which we predict the class of an observation is a decision the user needs to make. While in some cases a reasonable probability threshold is 50%, in other cases it may be 99.9% or 0.01%. Can you think of such cases?
</p> </blockquote>

For different choices of the probability threshold, one can measure a number of classification performance metrics. Specifically: 

### 1.  **Hit ratio** 
This is simply the percentage of the observations that have been correctly classified (the posterior is the same as the prior classification). We can just count the number of the (first) validation data correctly classified and divide this number with the total number of the (fist) validation data, using the two CART and the logistic regression above. These are as follows for the probability threshold  `r Probability_Threshold*100`% for the validation data:

<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
validation_actual=validation_data[,dependent_variable]
validation_predictions = rbind(validation_prediction_class_tree,
                               validation_prediction_class_tree_large,
                               validation_prediction_class_log)
validation_hit_rates = rbind(
  100*sum(validation_prediction_class_tree==validation_actual)/length(validation_actual), 
  100*sum(validation_prediction_class_tree_large==validation_actual)/length(validation_actual), 
  100*sum(validation_prediction_class_log==validation_actual)/length(validation_actual)
  )
colnames(validation_hit_rates) <- "Hit Ratio"
rownames(validation_hit_rates) <- c("First CART", "Second CART", "Logistic Regression")

print(xtable(validation_hit_rates ,caption=paste("Validation Data Hit Ratios for different classifiers tested:",data_name,sep=" "), digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)

```
</div>
</div>

while for the estimation data the hit rates are:
<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
estimation_actual=estimation_data[,dependent_variable]
estimation_predictions = rbind(estimation_prediction_class_tree,
                               estimation_prediction_class_tree_large,
                               estimation_prediction_class_log)
estimation_hit_rates = rbind(
  100*sum(estimation_prediction_class_tree==estimation_actual)/length(estimation_actual), 
  100*sum(estimation_prediction_class_tree_large==estimation_actual)/length(estimation_actual), 
  100*sum(estimation_prediction_class_log==estimation_actual)/length(estimation_actual)
  )
colnames(estimation_hit_rates) <- "Hit Ratio"
rownames(estimation_hit_rates) <- c("First CART", "Second CART", "Logistic Regression")

print(xtable(estimation_hit_rates ,caption=paste("Estimation Data Hit Ratios for different classifiers tested:",data_name,sep=" "), digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)

```
</div>
</div>

**Why are the performances on the estimation and validation data different? How different can they possibly be? What does this diffference depend on?** Is the Validation Data Hit Rate satisfactory? Which classifier should we use? What should be the benchmark against which to compare the hit rate? There is one basic benchmark to be used: the Maximum Chance Criterion.

The **Maximum Chance Criterion** suggests that the hit rate should exceed the proportion of the class with the largest size. For our validation data the largest group is people who did not visit the resort: `r sum(validation_actual)` out of `r length(validation_actual)` people). Clearly without doing any discriminant analysis, if we classified all individuals into the largest group, then we could get a hit-rate of `r sum(validation_actual)/length(validation_actual)`% without doing any work. This is the Maximum Chance Criterion. Clearly, discriminant analysis should get a hit rate of at least the Maximum Chance Criterion. 

### 2. **Confusion matrix**

The confusion matrix shows the percentage of the estimation data that are put in the right group, for each group. For example for the method above with the highest hit rate in the validation data, the confusion matrix for the validation data is:


<div class="row">
<div class="col-md-6">
```{r echo=FALSE, message=FALSE, prompt=FALSE, results='asis'}
validation_prediction_best = validation_predictions[which.max(validation_hit_rates),]
cmp<-table(validation_data[,dependent_variable], validation_prediction_best,
           dnn=c("Actual", "Predicted"))
confp<-xtable(cmp,caption="Confusion matrix percentages for best Hit Rate classifier: Validation Data")
print(confp,type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top")

```

</div>
</div>
<br>


### 3.  **ROC curve** 

Remember that each observation is classified in a group by our model according to the probabilities Pr(0) and Pr(1). Typically we set the probability threshold to 0.5 - so that observations for which Pr(0) > 0.5 are classified as 0's. However, we can vary this threshold, for example if we are interested in correctly predicting all 1's but do not mind missing some 0's (and vice-versa) as may be the case of security alarms - what threshold for Pr(1) should we use in that case? When we change this threshold we get different values of hit rate, false positive, and false negative. We can plot the false positive versus false negative values we get this way, and generate the so called ROC curve. The proportion of well-classified positive events is called the sensitivity. The specificity is the proportion of well-classified negative events.  

The ROC curves for the validation data for both the CARTs above as well as the logistic regression are as follows:

```{r echo=FALSE,results='hide',include=FALSE,warning=FALSE,error=FALSE}

validation_actual_class <- as.numeric(validation_data[,dependent_variable])

pred_tree <- prediction(validation_Probability_class1_tree, validation_actual_class)
pred_tree_large <- prediction(validation_Probability_class1_tree_large, validation_actual_class)
pred_log <- prediction(validation_Probability_class1_log, validation_actual_class)

```
<center>ROC CURVE
```{r echo=FALSE, warning=FALSE,comment=NA, results='asis', message=FALSE, fig.align='center', fig=TRUE}
a<-split.screen(c(1, 3))
test<-performance(pred_tree, "tpr", "fpr")
df<- cbind(as.data.frame(test@x.values),as.data.frame(test@y.values))
colnames(df) <- c("False Positive rate CART", "True Positive CART")
Line    <- gvisLineChart(as.data.frame(df), xvar=c("False Positive rate CART"), yvar="True Positive CART", options=list(title='ROC Curve', legend="right", width=600, height=400, hAxis="{title:'False Positive rate CART', titleTextStyle:{color:'black'}}", vAxes="[{title:'True Positive CART'}]",  series="[{color:'green',pointSize:12, targetAxisIndex: 0}]"))
############################
test1<-performance(pred_log, "tpr", "fpr")
df1<- cbind(as.data.frame(test1@x.values),as.data.frame(test1@y.values))
colnames(df1) <- c("False Positive rate CART", "True Positive CART")
Line1    <- gvisLineChart(as.data.frame(df1), xvar=c("False Positive rate CART"), yvar="True Positive CART", options=list(title='ROC Curve for logistic regression', legend="right", width=600, height=400, hAxis="{title:'False Positive rate CART', titleTextStyle:{color:'black'}}", vAxes="[{title:'True Positive CART'}]",  series="[{color:'green',pointSize:12, targetAxisIndex: 0}]"))
###############################
test2<-performance(pred_tree_large, "tpr", "fpr")
df2<- cbind(as.data.frame(test2@x.values),as.data.frame(test2@y.values))
colnames(df2) <- c("False Positive rate CART", "True Positive CART")
Line2    <- gvisLineChart(as.data.frame(df2), xvar=c("False Positive rate CART"), yvar="True Positive CART", options=list(title='ROC Curve for CART large', legend="right", width=600, height=400, hAxis="{title:'False Positive rate CART', titleTextStyle:{color:'black'}}", vAxes="[{title:'True Positive CART'}]",  series="[{color:'green',pointSize:12, targetAxisIndex: 0}]"))
##############################3

screen(1)
print(Line, 'chart')
screen(2)
print(Line1, 'chart')
screen(3)
print(Line2, 'chart')
```

```{r echo=FALSE}
plot(performance(pred_tree_large, "tpr", "fpr"), col="red", lty=1, add=FALSE)
grid()
par(new=TRUE)
plot(performance(pred_log, "tpr", "fpr"), col="blue", lty=1, add=FALSE)
par(new=TRUE)
plot(performance(pred_tree, "tpr", "fpr"),  lty=1, add=FALSE, main="ROC Curve")
par(new=FALSE)

```
</center>
<br>
How should a good ROC curve look like? A rule of thumb in assessing ROC curves is that the "higher" the curve, hence the larger the area under the curve, the better. You may also select one point on the ROC curve (the "best one" for your purpose) and use that false positive/false negative performances (and corresponding threshold for P(0)) to assess your model. **Which point on the ROC should we select?**



### 4. **Lift curve**

By changing the threshold, we can also generate the so called lift curve, which is useful for certain applications. For example, consider the case of capturing fraud by examining only a few transactions instead of every single one. In this case we may want to examine as few transactions as possible and capture the maximum number of frauds possible. We can measure the percentage of all frauds we capture if we only examine, say, x% of cases (the top x% in terms of Probability(fraud)). If we plot these points [percentage captured vs total examined] while we change the threshold, we get a curve that is called the **lift curve**. 

The Lift curves for the validation data for our three classifiers are the following:


```{r lift,echo=FALSE,results='hide',include=FALSE,warning=FALSE,error=FALSE}

lift_tree <- performance(pred_tree, "lift")

lift_tree <- performance(pred_tree, "lift", "rpp")
lift_tree_large <- performance(pred_tree_large, "lift", "rpp")
lift_log <- performance(pred_log, "lift", "rpp")
lift_tree@x.values[[1]] <- lift_tree@x.values[[1]]*100
lift_tree_large@x.values[[1]] <- lift_tree_large@x.values[[1]]*100
lift_log@x.values[[1]] <- lift_log@x.values[[1]]*100
```

<center>LIFT CURVES
```{r echo=FALSE}
plot(lift_tree,  lty=1, xlab="% of data", add=FALSE, main="Lift Curve")
grid()
par(new=TRUE)
plot(lift_tree_large, col="red", lty=1, xlab="% of data", add=FALSE, main="Lift Curve")
par(new=TRUE)
plot(lift_log, col = "blue", lty=1, xlab="% of data", add=FALSE, main="Lift Curve")
par(new=FALSE)

```
</center>
<br>

How should a good Lift Curve look like? Notice that if we were to randomly examine transactions, **the "random prediction" lift curve would be a 45 degrees straight diagonal line** (why?)! So the further **above** this 45 degrees line our Lift curve is, the better the "lift". Moreover, much like for the ROC curve, one can select the probability threshold appropriately so that any point of the lift curve is selected. **Which point on the lift curve should we select?** 

###5. **Profit Curve** 

Finally, we can generate the so called profit curve, which we often use to make our final decisions.  The intuition, from a direct marketing example, is as follows. Suppose it costs $ 1 to send a mailing. Suppose the expected profit from a person who responds is $45. Suppose you have a database of 1 million people to whom you could potentially send the mailings. What fraction of the 1 million people should you send mails (typical response rates are 0.05%)? To answer this type of questions we need to create the profit curve, which is generated by changing again the probability threshold for classifying observations: for each threshold value we can simply measure the total **Expected Profit** (or loss) we would generate. This is simply equal to:

<blockquote> <p>
Total Expected Profit = (% of 1's correctly predicted)x(value of capturing a 1) + (% of 0's correctly predicted)x(value of capturing a 0) + (% of 1's incorrectly predicted as 0)x(cost of missing a 1) + (% of 0's incorrectly predicted as 1)x(cost of missing a 0)

Calculating the expected profit requires we have an estimate of the 4 costs/values: value of capturing a 1 or a 0, and cost of misclassifying a 1 into a 0 or vice versa. 
</p> </blockquote>

Given the values and costs of correct classifications and misclassifications, we can plot the total expected profit (or loss) as we change the probabibility threshold, much like how we generated the ROC and the Lift Curves. Here is the profit curve for our example if we consider the following business profit and loss for the correctly classified as well as the misclassified customers: 

<div class="row">
<div class="col-md-6">
```{r echo=FALSE,results='hide',include=FALSE,warning=FALSE,error=FALSE}
Profit_Matrix = matrix(c(100, -10, -100, 0), ncol=2)
colnames(Profit_Matrix)<- c("Predict 1", "Predict 0")
rownames(Profit_Matrix) <- c("Actual 1", "Actual 0")

print(xtable(Profit_Matrix ,caption=paste("Assumed Profits and Costs:",data_name,sep=" "), digits=1,),type="html",html.table.attributes = "class='table table-striped table-hover table-bordered'",caption.placement="top",comment = FALSE,include.rownames = TRUE)
```
</div>
</div>
Based on these numbers, the profit curves for the validation data for the three classifiers are:
<center>
```{r echo=FALSE,results='hide',include=FALSE,warning=FALSE,error=FALSE}

prob_tree = validation_Probability_class1_tree
prob_tree_large = validation_Probability_class1_tree_large
prob_log = validation_Probability_class1_log

actual_class = validation_actual_class

xaxis = 0.01*(0:100)

yaxis_tree = sapply(xaxis, function(thres){
  predict_class = 1*(prob_tree > thres)
  Profit_Matrix[1,1]*sum(predict_class==1 & actual_class ==1)+
    Profit_Matrix[1,2]*sum(predict_class==0 & actual_class ==1)+
    Profit_Matrix[2,1]*sum(predict_class==1 & actual_class ==0)+
    Profit_Matrix[2,2]*sum(predict_class==0 & actual_class ==0)
})

yaxis_tree_large = sapply(xaxis, function(thres){
  predict_class = 1*(prob_tree_large > thres)
  Profit_Matrix[1,1]*sum(predict_class==1 & actual_class ==1)+
    Profit_Matrix[1,2]*sum(predict_class==0 & actual_class ==1)+
    Profit_Matrix[2,1]*sum(predict_class==1 & actual_class ==0)+
    Profit_Matrix[2,2]*sum(predict_class==0 & actual_class ==0)
})

yaxis_log = sapply(xaxis, function(thres){
  predict_class = 1*(prob_log > thres)
  Profit_Matrix[1,1]*sum(predict_class==1 & actual_class ==1)+
    Profit_Matrix[1,2]*sum(predict_class==0 & actual_class ==1)+
    Profit_Matrix[2,1]*sum(predict_class==1 & actual_class ==0)+
    Profit_Matrix[2,2]*sum(predict_class==0 & actual_class ==0)
})



```
<center>
We can then simply select the threshold that corresponds to the maximum profit (or minimum loss).

Notice that for this we need to have the cost/value for each of the 4 cases! This can be difficult to assess, hence typically some sensitivity analysis to our assumptions about the cost/value needs to be done: we can generate different profit curves (i.e. worst case, best case, average case scenario) and see how much the best profit we get varies. 



#### Step 6: Finally, assess the accuracy of classification in the second validation sample.

Having iterated steps 2-5 until we are satisfyed with the performance of our model on the validation data, in this step the performance analysis outlined in step 5 needs to be done with the second validation (the test) sample. This is the performance that "mimics" what one should expect in practice upon deployment of the classification solution, assuming (as always) that the data used for this performance analysis are representative of the situation in which the solution will be deployed. 

Let's see in our case how the **Confusion Matrix, ROC Curve, Lift Curve, and Profit Curve** look like for our test data:

<center>ROC CURVE
```{r echo=FALSE}


plot(performance(pred_tree_large, "tpr", "fpr"), col="red", lty=1, add=FALSE)
grid()
par(new=TRUE)
plot(performance(pred_log, "tpr", "fpr"), col="blue", lty=1, add=FALSE)
par(new=TRUE)
plot(performance(pred_tree, "tpr", "fpr"),  lty=1, add=FALSE, main="ROC Curve")
par(new=FALSE)

```
</center>
<br>
<center>PROFIT CURVES
```{r echo=FALSE}
plot(xaxis, yaxis_tree, type="l")
lines(xaxis, yaxis_tree_large, type="l", col="red")
lines(xaxis, yaxis_log, type="l", col="blue")
grid()
```

**Is the performance what we would expect given the performance in the validation data? Should the performance in the test data be alsways close to that in the validation data? More important: should we expect the performance of our classification model to be close to that in our test data when we deploy the model in practice? Why or why not?**


Of course, as always, remember that 

<blockquote> <p>
Data Analytics is an iterative process, therefore we may need to return to our original raw data at any point and select new raw attributes as well as a different classification solution.
</p> </blockquote>

**Till then...**
